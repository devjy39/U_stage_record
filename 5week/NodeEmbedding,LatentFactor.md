# 정점 표현 학습 - 정점 임베딩(Node Embedding)
- 그래프의 정점들을 벡터의 형태로 표현하는 것
- `임베딩 공간` :  정점이 표현되는 벡터 공간
- 그래프를 벡터 형태로 나타낼 수 있으면 다양한 기계학습 도구들에 적용이 가능해진다.
- 목표: 정점간 유사도를 임베딩 공간에서도 “보존”하는 것

## 유사도  𝒛𝒗⊺⋅𝒛𝒖 = ||𝒛𝒖|| ⋅ ||𝒛𝒗|| ⋅ 𝒄𝒐𝒔 𝜽 
- 임베딩 공간에서의 유사도로는 내적(Inner Product)
- 두 벡터가 클 수록, 그리고 같은 방향을 향할 수록 큰 값을 갖음.
1.  그래프에서의 정점 유사도를 정의
2.  정의한 유사도를 보존하도록 정점 임베딩을 학습

## 인접성 기반 접근법
- 두 정점이 간선으로 연결 되어있을 때 유사하다 간주
- 손실 함수(Loss Function)
<img src=image/adjacencyloss.PNG>
 
- 손실 함수 최소화를 위해서는 (확률적) 경사하강법 등이 사용
### ※그래프 상의 거리와 군집 관계를 무시하게 되는 한계가 있음.

### 거리 기반 접근법
- 두 정점 사이의 거리가 일정 수만큼 가까운 경우 유사함.

### 경로 기반 접근법
- 경로의 연속된 정점은 간선으로 연결되어 있어야 한다.
- 두 정점 사이의 경로가 많을 수록 유사하다.
- 손실 함수(Loss Function)
<img src=image/pathloss.PNG>

###  중첩 기반 접근법
- 두 정점이 많은 이웃을 공유할수록 유사.
- 손실 함수(Loss Function)
<img src=image/nestingloss.PNG>
 
대체 기법으로
### 자카드 유사도(Jaccard Similarity)
- 공통 이웃의 수 대신 비율을 계산하는 방식
- 전체 이웃 중의 공통 이웃이 최대일 경우 1이 됨
### Adamic Adar 점수
- 공통 이웃 각각에 가중치를 부여하여 가중합을 계산
- 이웃이 많은 노드일수록 하나 하나 간선의 가치는 떨어지게 된다.

# 임의보행 기반 접근법
- 한 정점에서 시작하여 임의보행을 할 때 다른 정점에 도달할 확률
- 균일한 확률로 이웃 노드로 이동
- 거리를 제한하지 않기 때문에 그래프 전역 정보를 고려한다고 할 수 있음.
1. 각 정점에서 시작해 임의보행 반복
2. 임의보행 중 도달한 정점들을 리스트로 만듬
- P 도달확률 값이 크면 클수록 좋음
<img src=image/walkloss.PNG>
 
#### 도달 확률  𝑃(𝑣|𝑧𝑢)
<img src=image/arriveloss.PNG>

### DeepWalk
- 기본적인 임의보행
- 이웃 중 하나를 균일한 확률로 선택하는 이동하는 과정
 
## Node2Vec
- 2차 치우친 임의보행(Second-order Biased Random Walk)
- 현재 정점(예시에서 𝑣)과 직전에 머물렀던 정점(예시에서 𝑢)을 모두 고려하여 다음 정점을 선택
- 멀어지는 방향에 높은 확률을 부여한 경우, 정점의 역할(다리 역할, 변두리 정점 등)이 같은 경우 임베딩 된다.
- 가까워지는 방향에 높은 확률을 부여한 경우, 같은 군집(Community)에 속한 경우 임베딩이 된다.
- 손실함수는 계산에 정점의 수의 제곱에 비례해 너무 크기 때문에 `근사식`을 사용한다.
- 손실 함수(Loss Function)
<img src=image/node2vec.PNG>
 
- 모든 정점에 대해서 정규화하는 대신 연결성에 비례하는 확률로 네거티브 샘플을 뽑아서 비교

## 변환식 정점 표현 학습 VS 귀납식 정점 표현 학습
- 위에서 설명한 `변환식(Transdctive)` 방법은 학습의 결과로 정점의 임베딩 자체를 얻는다
- 변환식 임베딩 방법은 여러 한계
1. 학습이 진행된 이후에 추가된 정점에 대해서는 임베딩을 얻을 수 없습니다
2. 모든 정점에 대한 임베딩을 미리 계산하여 저장해두어야 합니다
3. 정점이 속성(Attribute) 정보를 가진 경우에 이를 활용할 수 없습니다

- `귀납식(Inductive)` 방법은 정점을 임베딩으로 변화시키는 함수, 즉 인코더를 얻는 방식
- 대표적인 귀납식 임베딩 방법이 바로 그래프 신경망(Graph Neural Network) 

## 잠재 인수 모형(Latent Factor Model)
- 사용자와 상품을 벡터로 표현하는 것
- 고정된 인수 대신 효과적인 인수를 학습하는 것
<img src=image/LatentFactor.PNG>
 
#### 임베딩의 목표는 사용자와 상품의 임베딩의 내적(Inner Product)이 평점과 최대한 유사하도록 하는 것
- 손실 함수(Loss Function)
<img src=image/latentloss.PNG>
 
- 과적합(Overfitting)을 방지하기 위하여 정규화 항 추가

## 고급 잠재 인수 모형
- 사용자의 편향: 해당 사용자의 평점 평균과 전체 평점 평균의 차
-  평점을 전체 평균, 사용자 편향, 상품 편향, 상호작용으로 분리하여 성능을 끌어올림
<img src=image/advancedloss.PNG>
 
- 성능을 더 끌어올리기 위해 `시간적 편향`도 고려
- ex) 영화의 평점은 출시일 이후 시간이 지남에 따라 상승하는 경향이 있음

### 넷플릭스 챌린지
- 잠재 인수 모형을 만들어냄
- 추천시스템 성장에 큰 도움이 되는 사건.


### 조교님 말씀 메모
- pycaret :  정형데이터 처리툴
- 큰 기업은 논문 구현능력이 중요.
